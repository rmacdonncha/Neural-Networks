{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # ARTIFICAL NEURAL NETWORKS\n",
    "\n",
    "Below is an example of how to  create an ANN in Python. In this case, the data comes from the following link, which you can download as a csv. \n",
    "https://docs.google.com/spreadsheets/d/18-kNzW_smJjpNChsrzLBzzYKGJYcnDwbpt-FuJEyVzA/edit?usp=sharing",
    "With this data the below ANN will build a modelto predict whether a customer will \n",
    "churn (yes/no)\n",
    "However, this template can be used for other ANN tasks too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 1: DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding categorical data\n",
    "Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 3 dummy variables from index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing one of these dummy variables to avoid the dummy varaible trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: BUILDING THE ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Keras packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential #used to initialise our model\n",
    "from keras.layers import Dense #used to create the layers in our ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the ANN as a sequence of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 6,# number of nodes in hidden layer. tip; use avg. of no. nodes in input and output layer (in this case 11 & 1, which is 6)\n",
    "                     kernel_initializer ='uniform', #initial weights, uniform function sets them randomly where they are close to, but not quite 0\n",
    "                     activation = 'relu',#activiation function, in this case we will use recitifier ala 'relu' for hidden layer\n",
    "                     input_dim = 11 #because we have 11 independant variables\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 6,\n",
    "                     kernel_initializer ='uniform', \n",
    "                     activation = 'relu'\n",
    "                     #because we have created the first hidden layer, we don't have to code input dimensions for subsequent hidden layers\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 1, #output layer will be 1 node, becse it is categotical with a binary outcome\n",
    "                     kernel_initializer ='uniform', \n",
    "                     activation = 'sigmoid'#sigmoid functions works well for generating probabilities in the output layer\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', #aka stochastic gradient descent\n",
    "                   loss = 'binary_crossentropy',#the loss function. Because we use a sigmoid function in output layer we'll use a logarithmic fuction (cross-entropy)\n",
    "                   metrics = ['accuracy']#criterion used to evaluate your model, typically we use 'accuracy'. as it can be a list we need to use square brackets\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the ANN to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train,\n",
    "               y_train,\n",
    "               batch_size = 10, #no rule of thumb on correct number\n",
    "               epochs = 50  #no rule of thumb on correct number\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: MAKING THE PREDICTIONS AND EVALUATING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5) #equivalent of saying if y-pred >0.5 then true, else false aka (1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting a new single new observation\n",
    "\"\"\"Predict if a customer with the folling info will leave bank:\n",
    "    Geo: France\n",
    "    Credit Score: 600\n",
    "    Gender: Male\n",
    "    Age: 40\n",
    "    Tenure: 3\n",
    "    Balance: 60000\n",
    "    No. Products: 2\n",
    "    Has Credit Card:Yes\n",
    "    Is Active Memeber:Yes\n",
    "    Estimated Salary: 50000\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction = classifier.predict(sc.transform(\n",
    "                                    np.array([[ #when using numpy array we need to use double quare brackets for horizontal arrays\n",
    "                                                0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000 #in the same order/format as our other data\n",
    "                                                ]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4:EVALUATING, IMPROVING AND TUNING THE ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the ANN\n",
    "We will use K-Fold Cross Validation to evaluate the variance and biases in our models's predicitive power\n",
    "cross_val_score function will bring back the 10 accuracies of the 10 cross folds made on out test set\n",
    "this function is only in skikit_learn so we will need to use keras wrapper to intergrate it to our keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from keras.models import Sequential # Used to initialise our model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout # See section on improving ANN below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that will build the ANN we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(): \n",
    "                        classifier = Sequential()\n",
    "                        classifier.add(Dense(units = 6,\n",
    "                                             kernel_initializer ='uniform', \n",
    "                                             activation = 'relu',\n",
    "                                             input_dim = 11 \n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dropout if needed (add to each layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  classifier.add(Dropout(p =  0.1,# The fraction of neurons you want to disable at each iteration\n",
    "                                               ))\n",
    "                        \n",
    "                        \n",
    "                        classifier.add(Dense(units = 6,\n",
    "                                             kernel_initializer ='uniform', \n",
    "                                             activation = 'relu'\n",
    "                                             ))\n",
    "                        classifier.add(Dense(units = 1, \n",
    "                                             kernel_initializer ='uniform', \n",
    "                                             activation = 'sigmoid'\n",
    "                                             ))\n",
    "                        classifier.compile(optimizer = 'adam',\n",
    "                                           loss = 'binary_crossentropy',\n",
    "                                           metrics = ['accuracy']\n",
    "                                           )\n",
    "                        return classifier # not only builds classifier but also returns it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = build_classifier,\n",
    "                             batch_size = 10,\n",
    "                             epochs = 100\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below will bring back the 10 accuracies created by the cross fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies =  cross_val_score(estimator = classifier, # object used to fit the data, which is our classifier\n",
    "                              X = X_train, # the data to fit\n",
    "                              y = y_train, # the target variable to try and predict\n",
    "                              cv = 10, # the number of cross-folds, no real rule of thumb but 10 is a good number\n",
    "                              n_jobs = 1 # the number of cpus to use to do the computation. -1 means all CPU's, so faster computation\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average prediction accuracy of the 10 k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the ANN:\n",
    "Drop out regularization to reduce overfitting if needed. Overfitting could be determined by hgh variance in the k-fold prediction results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the ANN:\n",
    "If we want to improve the prediction rate of our model, we can use prarmaeter tuning. \n",
    "There are 2 types of parameter in our model: learned paramters (weights) and hyperparamters thatn stay fixed (# epochs/neurons, batch sizes, optimisers etc..)\n",
    "We will use GridSeach to test several combinations of these parameters and bring back the best selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "def build_classifier(optimizer): \n",
    "                        classifier = Sequential()\n",
    "                        classifier.add(Dense(units = 6,\n",
    "                                             kernel_initializer ='uniform', \n",
    "                                             activation = 'relu',\n",
    "                                             input_dim = 11 \n",
    "                                             ))\n",
    "                        classifier.add(Dense(units = 6,\n",
    "                                             kernel_initializer ='uniform', \n",
    "                                             activation = 'relu'\n",
    "                                             ))\n",
    "                        classifier.add(Dense(units = 1, \n",
    "                                             kernel_initializer ='uniform', \n",
    "                                             activation = 'sigmoid'\n",
    "                                             ))\n",
    "                        classifier.compile(optimizer = 'adam',\n",
    "                                           loss = 'binary_crossentropy',\n",
    "                                           metrics = ['accuracy']\n",
    "                                           )\n",
    "                        return classifier \n",
    "                    \n",
    "classifier = KerasClassifier(build_fn = build_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a 'dictionary' of all the hyperparameters that the gridsearch will try combiniationsof and return the the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'batch_size':[25, 32],\n",
    "              'epochs':[100, 500],\n",
    "              'optimizer':['adam','rmsprop']\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will still use k-fold cross-validation to ensure we aren't biased and that there isn't too much variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                          param_grid = parameters,\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting it to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = grid_search.fit(X_train,\n",
    "                              y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for the best parameters and best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = gird_search.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
